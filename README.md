Hybrid RAG Q&A System with Confidence Threshold + Tools

This project implements a Retrieval-Augmented Generation (RAG) pipeline using LangChain, Groqâ€™s Llama3 model, and external knowledge sources like Arxiv, Wikipedia, and DuckDuckGo Search.

If a userâ€™s question is relevant to local documents, the system answers using the vector database (RAG).
If not, it automatically switches to external tools for real-time web or academic retrieval â€” achieving a hybrid intelligence workflow.

ðŸš€ Features

âœ… RAG-based Q&A using FAISS Vector Store
âœ… Confidence Thresholding to decide between internal vs external knowledge
âœ… External Tool Integration (Arxiv, Wikipedia, DuckDuckGo Search)
âœ… Groq-powered LLM (Llama3-8b) for fast, low-latency responses
âœ… Streamlit UI for interactive exploration
âœ… Automatic document ingestion and embedding creation

ðŸ§© Architecture Overview
User Query
   â”‚
   â–¼
 Check Vector Similarity (FAISS)
   â”œâ”€â”€ High Confidence â†’ RAG Answer (contextual docs)
   â””â”€â”€ Low Confidence â†’ Use Tools (Arxiv, Wikipedia, DuckDuckGo)
