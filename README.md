Hybrid RAG Q&A System with Confidence Threshold + Tools

This project implements a Retrieval-Augmented Generation (RAG) pipeline using LangChain, Groqâ€™s Llama3 model, and external knowledge sources like Arxiv, Wikipedia, and DuckDuckGo Search.

If a userâ€™s question is relevant to local documents, the system answers using the vector database (RAG).
If not, it automatically switches to external tools for real-time web or academic retrieval â€” achieving a hybrid intelligence workflow.

ðŸš€ Features

âœ… RAG-based Q&A using FAISS Vector Store
âœ… Confidence Thresholding to decide between internal vs external knowledge
âœ… External Tool Integration (Arxiv, Wikipedia, DuckDuckGo Search)
âœ… Groq-powered LLM (Llama3-8b) for fast, low-latency responses
âœ… Streamlit UI for interactive exploration
âœ… Automatic document ingestion and embedding creation

ðŸ§© Architecture Overview
<img width="1024" height="1536" alt="Hybrid Question Answering System Flowchart" src="https://github.com/user-attachments/assets/0c25f303-9c6a-4a71-a259-ebe769269268" />

